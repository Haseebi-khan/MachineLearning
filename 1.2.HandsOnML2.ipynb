{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3100ee2",
   "metadata": {},
   "source": [
    "you want to know how happy Cypriots are, and the OECD data does not have the answer.\n",
    "Fortunately, you can use your model to make a good prediction: you look up Cyprus’s\n",
    "GDP per capita, find $22,587, and then apply your model and find that life satisfac‐\n",
    "tion is likely to be somewhere around 4.85 + 22,587 × 4.91 × 10-5 = 5.96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "oecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=',')\n",
    "gdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n",
    " encoding='latin1', na_values=\"n/a\")\n",
    "# Prepare the data\n",
    "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.show()\n",
    "# Select a linear model\n",
    "lin_reg_model = sklearn.linear_model.LinearRegression()\n",
    "# Train the model\n",
    "lin_reg_model.fit(X, y)\n",
    "# Make a prediction for Cyprus\n",
    "X_new = [[22587]] # Cyprus' GDP per capita\n",
    "print(lin_reg_model.predict(X_new)) # outputs [[ 5.96242338]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a108bd",
   "metadata": {},
   "source": [
    "If you zoom out a bit and look at the\n",
    "two next closest countries, you will find Portugal and Spain with\n",
    "life satisfactions of 5.1 and 6.5, respectively. Averaging these three\n",
    "values, you get 5.77, which is pretty close to your model-based pre‐\n",
    "diction. This simple algorithm is called k-Nearest Neighbors regres‐\n",
    "sion (in this example, k = 3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f96b8d",
   "metadata": {},
   "source": [
    "The idea that data matters more than algorithms for complex problems was further\n",
    "popularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness\n",
    "of Data” published in 2009.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a530f1c",
   "metadata": {},
   "source": [
    "The Unreasonable Effectiveness of Data\n",
    "\n",
    "Nonrepresentative Training Data\n",
    "\n",
    "A Famous Example of Sampling Bias\n",
    "\n",
    "Poor-Quality Data\n",
    "\n",
    "Irrelevant Features\n",
    "\n",
    "Overfitting the Training Data\n",
    "\n",
    "Constraining a model to make it simpler and reduce the risk of overfitting is called\n",
    "regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7da6ef",
   "metadata": {},
   "source": [
    "For example, the linear model we defined earlier has two parameters,\n",
    "θ0 and θ1\n",
    ". This gives the learning algorithm two degrees of freedom to adapt the model\n",
    "to the training data: it can tweak both the height (θ0\n",
    ") and the slope (θ1\n",
    ") of the line. If\n",
    "we forced θ1\n",
    " = 0, the algorithm would have only one degree of freedom and would\n",
    "have a much harder time fitting the data properly: all it could do is move the line up\n",
    "or down to get as close as possible to the training instances,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd231f",
   "metadata": {},
   "source": [
    "Regularization reduces the risk of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97821bf8",
   "metadata": {},
   "source": [
    "Underfitting the Training Data\n",
    "\n",
    "As you might guess, underfitting is the opposite of overfitting: it occurs when your\n",
    "model is too simple to learn the underlying structure of the data. For example, a lin‐\n",
    "ear model of life satisfaction is prone to underfit; reality is just more complex than\n",
    "the model, so its predictions are bound to be inaccurate, even on the training exam‐\n",
    "ples.\n",
    "\n",
    "The main options to fix this problem are:\n",
    "\n",
    "• Selecting a more powerful model, with more parameters\n",
    "\n",
    "• Feeding better features to the learning algorithm (feature engineering)\n",
    "\n",
    "• Reducing the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a99e2",
   "metadata": {},
   "source": [
    "# End-to-End Machine Learning Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e097f4",
   "metadata": {},
   "source": [
    "• Popular open data repositories:\n",
    "\n",
    "— UC Irvine Machine Learning Repository\n",
    "\n",
    "— Kaggle datasets\n",
    "\n",
    "— Amazon’s AWS datasets\n",
    "\n",
    "• Meta portals (they list open data repositories):\n",
    "\n",
    "— http://dataportals.org/\n",
    "\n",
    "— http://opendatamonitor.eu/\n",
    "\n",
    "— http://quandl.com/\n",
    "\n",
    "• Other pages listing many popular open data repositories:\n",
    "\n",
    "— Wikipedia’s list of Machine Learning datasets\n",
    "\n",
    "— Quora.com question\n",
    "\n",
    "— Datasets subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0169a0c",
   "metadata": {},
   "source": [
    "Pipelines\n",
    "\n",
    "A sequence of data processing components is called a data pipeline. Pipelines are very\n",
    "common in Machine Learning systems, since there is a lot of data to manipulate and\n",
    "many data transformations to apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62acdba1",
   "metadata": {},
   "source": [
    "https://github.com/ageron/handson-ml."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3627d69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a test set is theoretically quite simple: just pick some instances randomly,\n",
    "# typically 20% of the dataset, and set them aside:\n",
    "import numpy as np\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "    # You can then use this function like this:\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(len(train_set), \"train +\", len(test_set), \"test\")\n",
    "# 16512 train + 4128 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b29ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199ba365",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
